{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702953ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Creating optimizer\n",
      "Using device: cuda\n",
      "Epoch 1/5.. Train loss: 4.579.. Validation loss: 4.415.. Validation accuracy: 0.106\n",
      "Epoch 1/5.. Train loss: 4.406.. Validation loss: 4.193.. Validation accuracy: 0.152\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/cd0673/43ef1733-cf74-40a9-8c22-b6e440128200/image-classifier-part-2-workspace/train.py\", line 92, in <module>\n",
      "    train_model(args.data_dir, args.arch, args.hidden_units, args.learning_rate, args.epochs, args.gpu, args.save_dir)\n",
      "  File \"/workspace/cd0673/43ef1733-cf74-40a9-8c22-b6e440128200/image-classifier-part-2-workspace/train.py\", line 32, in train_model\n",
      "    for inputs, labels in train_loader:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 231, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py\", line 172, in to_tensor\n",
      "    img = img.permute((2, 0, 1)).contiguous()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py /workspace/cd0673/43ef1733-cf74-40a9-8c22-b6e440128200/image-classifier-part-1-workspace/home/aipnd-project/flowers --arch vgg16 --hidden_units 512 --learning_rate 0.0001 --epochs 5 --gpu --save_dir checkpoints_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffea8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Creating optimizer\n",
      "Using device: cuda\n",
      "Epoch 1/1.. Train loss: 4.857.. Validation loss: 4.089.. Validation accuracy: 0.179\n",
      "Epoch 1/1.. Train loss: 4.129.. Validation loss: 3.368.. Validation accuracy: 0.272\n",
      "Epoch 1/1.. Train loss: 3.440.. Validation loss: 2.943.. Validation accuracy: 0.350\n",
      "Epoch 1/1.. Train loss: 3.202.. Validation loss: 2.483.. Validation accuracy: 0.450\n",
      "Epoch 1/1.. Train loss: 3.109.. Validation loss: 2.231.. Validation accuracy: 0.450\n",
      "Epoch 1/1.. Train loss: 2.878.. Validation loss: 1.841.. Validation accuracy: 0.545\n",
      "Epoch 1/1.. Train loss: 2.405.. Validation loss: 1.754.. Validation accuracy: 0.548\n",
      "Epoch 1/1.. Train loss: 2.051.. Validation loss: 1.527.. Validation accuracy: 0.613\n",
      "Epoch 1/1.. Train loss: 1.972.. Validation loss: 1.299.. Validation accuracy: 0.665\n",
      "Epoch 1/1.. Train loss: 1.990.. Validation loss: 1.119.. Validation accuracy: 0.713\n",
      "Epoch 1/1.. Train loss: 1.911.. Validation loss: 1.016.. Validation accuracy: 0.735\n",
      "Epoch 1/1.. Train loss: 1.631.. Validation loss: 1.042.. Validation accuracy: 0.712\n",
      "Epoch 1/1.. Train loss: 1.961.. Validation loss: 1.072.. Validation accuracy: 0.708\n",
      "Epoch 1/1.. Train loss: 1.560.. Validation loss: 0.909.. Validation accuracy: 0.764\n",
      "Epoch 1/1.. Train loss: 1.572.. Validation loss: 0.875.. Validation accuracy: 0.788\n",
      "Epoch 1/1.. Train loss: 1.598.. Validation loss: 0.855.. Validation accuracy: 0.770\n",
      "Epoch 1/1.. Train loss: 1.701.. Validation loss: 0.848.. Validation accuracy: 0.766\n",
      "Epoch 1/1.. Train loss: 1.445.. Validation loss: 0.836.. Validation accuracy: 0.768\n",
      "Epoch 1/1.. Train loss: 1.604.. Validation loss: 0.758.. Validation accuracy: 0.801\n",
      "Epoch 1/1.. Train loss: 1.476.. Validation loss: 0.778.. Validation accuracy: 0.793\n",
      "Model training completed successfully\n",
      "Model checkpoint saved at checkpoints_dir/checkpointpart2.pth\n",
      "Model checkpoint saved at checkpoints_dir/checkpointpart2.pth\n"
     ]
    }
   ],
   "source": [
    "!python train.py /workspace/cd0673/43ef1733-cf74-40a9-8c22-b6e440128200/image-classifier-part-1-workspace/home/aipnd-project/flowers --arch vgg16 --hidden_units 512 --learning_rate 0.001 --epochs 1 --gpu --save_dir checkpoints_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdae56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
